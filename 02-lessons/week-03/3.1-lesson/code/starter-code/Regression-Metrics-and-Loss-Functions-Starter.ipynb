{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Metrics and Loss Functions\n",
    "\n",
    "We've seen two examples of _loss functions_ earlier in the week in the context of regularization:\n",
    "* The sum of squared errors:\n",
    "$$\\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2}$$\n",
    "* Regularlized versions\n",
    "$$\\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2 + \\alpha \\theta_i}$$\n",
    "\n",
    "In this lesson we're going to dig deeper into loss functions and their applications. Different loss functions are useful in different scenarios and there are two very popular loss functions that are used in conjuction with regression. In this case they are sometimes referred to as _regression metrics_.\n",
    "\n",
    "The first is the _root mean squared error_ or _RMSE_ and it is the mean of the squared errors. If we have $n$ regression points and their predictions, the [RMSE](https://en.wikipedia.org/wiki/Root-mean-square_deviation) is:\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\frac{\\sum_{i}{\\left(\\hat{y}_i - y_i \\right)^2}}{n}}$$\n",
    "\n",
    "The second is the _mean absolute error_ or _MAE_, and it differs by use of an absolute value instead of a square. The [MAE](https://en.wikipedia.org/wiki/Average_absolute_deviation) is:\n",
    "\n",
    "$$\\text{MAE} = \\frac{\\sum_{i}{|\\hat{y}_i - y_i |}}{n}$$\n",
    "\n",
    "## Why have different regression metrics?\n",
    "\n",
    "You might be thinking, _what's all the fuss about_? It turns out that there are lots of good reasons to use different loss functions. We've seen one -- regularization -- and now we'll consider the effects of outliers on these two metrics.\n",
    "\n",
    "First let's try a very simplified statistics problem. Given a dataset, how can we summarize it with a single number? Do you know any ways?\n",
    "\n",
    "This is equivalent to fitting a constant model to the data. It turns out that the _mean_ minimizes the RMSE and the _median_ minimizes the MAE. By analogy, when fitting a model, MAE is more tolerant to outliers. In other words, the degree of error of an outlier has a large impact when using RMSE versus the MAE. Since the choice of loss function affects model fit, it's important to consider how you want errors to impact your models.\n",
    "\n",
    "**Summary**\n",
    "* Use MAE when how far off an error is makes little difference\n",
    "* Use RMSE when more extreme errors should have a large impact\n",
    "\n",
    "Finally, note that linear regressions with MAE instead of RMSE are called _least absolute deviation_ regressions rather than least squares regressions.\n",
    "\n",
    "### Bonus: Modes\n",
    "\n",
    "It turns out the _mode_ minimizes the sum:\n",
    "$$\\frac{\\sum_{i}{|\\hat{y}_i - y_i |^{0}}}{n}$$\n",
    "where $0^0=0$ and $x^0=1$ otherwise. Can you see why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided practice\n",
    "\n",
    "Let's compute the RMSE and the MAE for a sample data set. Let's say we had a quadratic function that we fit a line to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 5, 10]\n",
      "[-2, 0, 2, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "xs = [-1, 0, 1, 2, 3]\n",
    "ys = [x*x + 1 for x in xs] # true values\n",
    "predictions = [2*x for x in xs]\n",
    "print ys\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First do the calculation by hand to see how large each term is\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 2.60768096208\n",
      "MAE: 2.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "print \"RMSE:\", (mean_squared_error(ys, predictions))**.5\n",
    "print \"MAE:\", mean_absolute_error(ys, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add an outlier to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 5.81664278887\n",
      "MAE: 3.83333333333\n"
     ]
    }
   ],
   "source": [
    "xs.append(4)\n",
    "ys.append(17)\n",
    "predictions.append(30)\n",
    "\n",
    "print \"RMSE:\", (mean_squared_error(ys, predictions))**.5\n",
    "print \"MAE:\", mean_absolute_error(ys, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the impact on the RMSE was large, a factor of 8, versus the impact on the MAE with a factor of 2.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indepedent Practice\n",
    "\n",
    "Let's explore two scenarios to obtain a better understanding of RMSE and MAE. First let's fit two models to the same set of data, the data above. To do the least mean absolute error we will use `statsmodels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# Make the plots bigger\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         QuantReg Regression Results                          \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   Pseudo R-squared:               0.5556\n",
      "Model:                       QuantReg   Bandwidth:                         nan\n",
      "Method:                 Least Squares   Sparsity:                          nan\n",
      "Date:                Wed, 22 Jun 2016   No. Observations:                    6\n",
      "Time:                        10:06:12   Df Residuals:                        4\n",
      "                                        Df Model:                            1\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.0000        nan        nan        nan           nan       nan\n",
      "x              3.0000        nan        nan        nan           nan       nan\n",
      "==============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelmainzer/anaconda/lib/python2.7/site-packages/statsmodels/stats/stattools.py:72: UserWarning: omni_normtest is not valid with less than 8 observations; 6 samples were given.\n",
      "  \"samples were given.\" % int(n))\n"
     ]
    }
   ],
   "source": [
    "#transpose those two arrays into a dataframe\n",
    "df = pd.DataFrame(np.array([xs, ys]).transpose(), columns=[\"x\", \"y\"])\n",
    "df.columns = [\"x\", \"y\"]\n",
    "mod = smf.quantreg('y ~ x', df)\n",
    "res = mod.fit(q=.5)\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generated a fit of $y = 4 x + 1$. Let's see what a linear regression yields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.808\n",
      "Model:                            OLS   Adj. R-squared:                  0.760\n",
      "Method:                 Least Squares   F-statistic:                     16.88\n",
      "Date:                Wed, 22 Jun 2016   Prob (F-statistic):             0.0148\n",
      "Time:                        10:06:12   Log-Likelihood:                -13.998\n",
      "No. Observations:                   6   AIC:                             32.00\n",
      "Df Residuals:                       4   BIC:                             31.58\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.6667      1.660      1.004      0.372        -2.942     6.276\n",
      "x1             3.0000      0.730      4.108      0.015         0.972     5.028\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   1.071\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.708\n",
      "Skew:                           0.382   Prob(JB):                        0.702\n",
      "Kurtosis:                       1.500   Cond. No.                         3.31\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = np.array(xs).transpose()\n",
    "X = sm.add_constant(X)\n",
    "# Fit and summarize OLS model\n",
    "mod = sm.OLS(ys, X)\n",
    "res = mod.fit()\n",
    "print res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yielded a fit of $y = 3.125 x + 1.5625$.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Plot the data with both functions. Which do you think fits the data better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1 = lambda x: 4*x + 1\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's explore another scenario. Linear regression has five major assumptions, one of which is called _constant variance_ or _homoscedasticity_. It means that the errors are distributed with the same variance about the best fit line regardless of the value of the independent variables.\n",
    "\n",
    "In practice this means that data with a persistant level of background noise can cause regression metrics to be poorly estimated. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFVCAYAAAA+OJwpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10410bb90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from scipy.stats import norm\n",
    "# Generate some data\n",
    "xs = list(np.arange(0, 10, 0.1))\n",
    "ys = [2*x + norm.pdf(0, 1) for x in xs]\n",
    "# Add random background noise\n",
    "xs2 = [10 * random.random() for i in range(20)]\n",
    "ys2 = [20 * random.random() for i in range(20)]\n",
    "\n",
    "# Plot the data sets\n",
    "plt.scatter(xs, ys, color='b')\n",
    "plt.scatter(xs2, ys2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine the data\n",
    "xs.extend(xs2)\n",
    "ys.extend(ys2)\n",
    "df = pd.DataFrame(np.array([xs, ys]).transpose(), columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit a line to the data\n",
    "# Compute the RMSE and the MAE\n",
    "# Plot the regression line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now try a MAE regression with statsmodels and plot it.\n",
    "# You should see a much better fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the data and the two fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
